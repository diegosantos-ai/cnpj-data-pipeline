# CNPJ Data Pipeline

## ğŸ“‹ VisÃ£o geral do projeto

Este projeto nasceu como uma resposta prÃ¡tica a um desafio real de engenharia de dados envolvendo os Dados Abertos de CNPJ da Receita Federal, que Ã© uma base pÃºblica, massiva e pouco amigÃ¡vel para uso analÃ­tico.

Mais do que construir um pipeline funcional, o objetivo foi lidar com decisÃµes reais de escopo, volume, integridade e trade-offs, comuns em ambientes de produÃ§Ã£o, mas raramente exploradas em projetos acadÃªmicos.

O projeto Ã© estruturado por fases, cada uma documentada e versionada, servindo como material de aprendizado prÃ¡tico e, principalmente, como evidÃªncia concreta da minha capacidade de atuar em engenharia de dados orientada a contexto e uso real.

---

## ğŸ¯ Objetivos do projeto

- Trabalhar com **dados pÃºblicos reais e volumosos**.
- Construir um pipeline **reprodutÃ­vel e organizado**.
- Aplicar boas prÃ¡ticas de engenharia de dados desde o setup.
- Gerar material utilizÃ¡vel como **portfÃ³lio profissional**.

---

## ğŸ›  Stack utilizada

- **Linguagem:** Python 3.13
- **Banco de Dados:** PostgreSQL 16
- **Infraestrutura:** Docker + Docker Compose
- **Bibliotecas:** SQLAlchemy, Pandas, python-dotenv, tqdm, requests
- **Ferramentas:** Adminer, Git e GitHub

---

## ğŸ“‚ Estrutura do projeto

```text
cnpj-data-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml       # Infraestrutura (Postgres + Adminer)
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ã£o central (DB e Pipeline)
â”‚   â”œâ”€â”€ paths.py             # CentralizaÃ§Ã£o de caminhos (DATA_ROOT)
â”‚   â”œâ”€â”€ bootstrap.py         # ValidaÃ§Ã£o de ambiente e diretÃ³rios
â”‚   â”œâ”€â”€ run_pipeline.py      # Orquestrador do pipeline
â”‚   â”œâ”€â”€ 00_test_connection.py # Teste de conexÃ£o
â”‚   â”œâ”€â”€ 01_download.py        # IngestÃ£o (Download)
â”‚   â”œâ”€â”€ 02_init_db.py         # InicializaÃ§Ã£o do schema
â”‚   â”œâ”€â”€ 03_extract_files.py   # ExtraÃ§Ã£o e Amostragem
â”‚   â””â”€â”€ 04_load_data.py       # Carga no banco de dados
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_tables.sql    # DDL das tabelas
â”œâ”€â”€ logs/                    # Logs de execuÃ§Ã£o
â””â”€â”€ docs/                    # DocumentaÃ§Ã£o e evidÃªncias
```

---

## ğŸ—ï¸ Fase 0 â€” Setup do ambiente (âœ… ConcluÃ­da)

**Objetivo:** Preparar um ambiente local totalmente reprodutÃ­vel, isolado via container e com ambiente Python controlado.

**Destaques:**
- PostgreSQL via Docker Compose.
- VariÃ¡veis de ambiente centralizadas no `.env`.
- Scripts de teste de conexÃ£o validados.

---

## ğŸ“¥ Fase 1 â€” IngestÃ£o de Dados (âœ… ConcluÃ­da)

### 1. Status da Fase
- **Status:** ConcluÃ­da
- **ValidaÃ§Ã£o:** QA aprovado (Sanity Checks 100% match em modo sample)
- **HD Externo:** Configurado e validado para grandes volumes.

### 2. CritÃ©rio de Encerramento
A Fase 1 foi encerrada apÃ³s o atendimento dos seguintes critÃ©rios:
- Pipeline de ingestÃ£o executÃ¡vel ponta a ponta.
- Paths externos (`DATA_ROOT`) configurados e isolados.
- Estrutura de `bootstrap` validada (Fail-fast para HD desconectado).
- Runner funcional (`run_pipeline.py`) com suporte a flags.
- **Suporte a modo `sample` inteligente** (preservando integridade referencial entre Empresas, Estabelecimentos e SÃ³cios).

### 3. DecisÃµes TÃ©cnicas Documentadas

#### 3.1 PadrÃ£o DATA_ROOT
Adotado para centralizar a localizaÃ§Ã£o de dados brutos e processados fora do repositÃ³rio Git, facilitando a portabilidade e mantendo o repositÃ³rio leve.

#### 3.2 Uso de HD Externo
DecisÃ£o consciente de arquitetura para lidar com o volume massivo da base completa (Big Data), garantindo escalabilidade sem comprometer o armazenamento interno (SSD).

#### 3.3 Modo Sample Inteligente
ImplementaÃ§Ã£o de amostragem ancorada em **Empresas**. O pipeline extrai uma amostra de empresas e filtra automaticamente os estabelecimentos e sÃ³cios correspondentes, garantindo que o banco de dados de teste seja consistente (Join Rate de 100%).

#### 3.4 Orquestrador (Runner)
CriaÃ§Ã£o do `src.run_pipeline` para centralizar a execuÃ§Ã£o, suportando as flags:
- `--mode [full|sample]`: Alterna entre carga completa e amostra.
- `--sample-rows N`: Define o tamanho da amostra.
- `--force`: ForÃ§a a regeraÃ§Ã£o de amostras.
- `--dry-run`: Simula as etapas sem execuÃ§Ã£o real.

### 4. EvidÃªncias de ExecuÃ§Ã£o

**ExecuÃ§Ã£o em modo Sample:**
```powershell
python -m src.run_pipeline --mode sample --sample-rows 50000 --force
```
- **Resultado:** ~500k registros carregados (50k por arquivo) com integridade referencial total.
- **Sanity Check:** Match rate Estabelecimentos -> Empresas: **100.0%**.

---

## ğŸ”œ PrÃ³xima fase: Fase 2 â€” TransformaÃ§Ã£o e NormalizaÃ§Ã£o
- Limpeza de dados.
- Tipagem correta de colunas (Datas, NÃºmeros).
- CriaÃ§Ã£o de Primary Keys e Ãndices para performance.
- NormalizaÃ§Ã£o de tabelas auxiliares (CNAEs, MunicÃ­pios, etc.).
