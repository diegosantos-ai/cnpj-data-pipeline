# CNPJ Data Pipeline

## ğŸ“‹ VisÃ£o geral do projeto

Este projeto tem como objetivo construir um **pipeline de engenharia de dados** utilizando os **Dados Abertos de CNPJ da Receita Federal**, cobrindo desde a preparaÃ§Ã£o do ambiente atÃ© a ingestÃ£o, modelagem e disponibilizaÃ§Ã£o dos dados para anÃ¡lise.

O projeto Ã© estruturado por **fases**, cada uma documentada e versionada, para servir tanto como **material de aprendizado prÃ¡tico** quanto como **evidÃªncia de experienciada aplicada em engenharia de dados**.

---

## ğŸ¯ Objetivos do projeto

- Trabalhar com **dados pÃºblicos reais e volumosos**.
- Construir um pipeline **reprodutÃ­vel e organizado**.
- Aplicar boas prÃ¡ticas de engenharia de dados (Quality Gates, Analytics Schema).
- Gerar material utilizÃ¡vel como **portfÃ³lio profissional**.

---

## ğŸ›  Stack utilizada

- **Linguagem:** Python 3.13
- **Banco de Dados:** PostgreSQL 16
- **Infraestrutura:** Docker + Docker Compose
- **Qualidade:** Great Expectations (GX)
- **Bibliotecas:** SQLAlchemy, Pandas, python-dotenv, tqdm, requests
- **Ferramentas:** Adminer, Git e GitHub

---

## ğŸ“‚ Estrutura do projeto

```text
cnpj-data-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml       # Infraestrutura (Postgres + Adminer)
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run_pipeline.py      # Orquestrador do pipeline (Runner)
â”‚   â”œâ”€â”€ bootstrap.py         # ValidaÃ§Ã£o de ambiente e diretÃ³rios
â”‚   â”œâ”€â”€ paths.py             # CentralizaÃ§Ã£o de caminhos (DATA_ROOT)
â”‚   â”œâ”€â”€ setup_gx.py          # ConfiguraÃ§Ã£o do Great Expectations
â”‚   â”œâ”€â”€ 01_download.py        # IngestÃ£o (Download)
â”‚   â”œâ”€â”€ 02_init_db.py         # InicializaÃ§Ã£o do schema public
â”‚   â”œâ”€â”€ 03_extract_files.py   # ExtraÃ§Ã£o e Amostragem Inteligente
â”‚   â”œâ”€â”€ 04_load_data.py       # Carga no banco de dados
â”‚   â”œâ”€â”€ 06_init_analytics_schema.py # CriaÃ§Ã£o do schema analytics
â”‚   â”œâ”€â”€ 07_promote_to_analytics.py  # PromoÃ§Ã£o Processed -> Analytics
â”‚   â””â”€â”€ 08_quality_gate.py    # Gate de Qualidade bloqueante (GX)
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql    # DDL das tabelas raw (public)
â”‚   â””â”€â”€ analytics/           # Scripts de promoÃ§Ã£o e views
â””â”€â”€ docs/                    # DocumentaÃ§Ã£o e evidÃªncias
```

---

## ğŸ—ï¸ Fase 0 â€” Setup do ambiente (âœ… ConcluÃ­da)

**Objetivo:** Preparar um ambiente local totalmente reprodutÃ­vel, isolado via container e com ambiente Python controlado.

---

## ğŸ“¥ Fase 1 â€” IngestÃ£o de Dados (âœ… ConcluÃ­da)

**Destaques:**
- **PadrÃ£o DATA_ROOT:** Armazenamento em drive externo para Big Data.
- **Modo Sample Inteligente:** Amostragem ancorada em Empresas com filtragem em cascata para Estabelecimentos e SÃ³cios, garantindo **100% de integridade referencial** mesmo em amostras pequenas.

---

## ğŸ“Š Fase 2 â€” Arquitetura Analytics & Qualidade (âœ… ConcluÃ­da)

### 1. PromoÃ§Ã£o Processed â†’ Analytics
Implementada a separaÃ§Ã£o fÃ­sica entre dados de processamento (`public`) e dados para consumo analÃ­tico (`analytics`).
- **Gate de Qualidade:** O script de promoÃ§Ã£o sÃ³ Ã© executado se os dados passarem nas validaÃ§Ãµes de integridade.
- **Views de Consumo:** CriaÃ§Ã£o de views analÃ­ticas otimizadas para dashboards.

### 2. Quality Gate com Great Expectations
IntegraÃ§Ã£o do **Great Expectations (GX 1.0+)** para garantir que apenas dados Ã­ntegros cheguem ao usuÃ¡rio final.
- **ValidaÃ§Ãµes:** Contagem de linhas, unicidade de CNPJ, obrigatoriedade de campos chave.
- **Data Docs:** DocumentaÃ§Ã£o automatizada da qualidade dos dados gerada a cada execuÃ§Ã£o.

---

## ğŸš€ Como Executar

### 1. Iniciar Infraestrutura
```powershell
docker-compose up -d
```

### 2. Rodar Pipeline (Modo Sample)
```powershell
python -m src.run_pipeline --mode sample --sample-rows 50000 --force
```

### 3. Validar e Promover
```powershell
python -m src.06_init_analytics_schema
python -m src.08_quality_gate
python -m src.07_promote_to_analytics
```

---

## ğŸ”œ PrÃ³xima fase: Fase 3 â€” TransformaÃ§Ã£o Pesada (Dask/DuckDB)
- Processamento paralelo para carga Full.
- ConversÃ£o para Parquet no HD externo.
- OtimizaÃ§Ã£o de performance para milhÃµes de registros.
